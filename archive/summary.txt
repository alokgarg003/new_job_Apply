Let me summarize the repository and produce a ready-to-copy README-style description plus an â€œAI promptâ€ template so an AI or developer can recreate the project from the prompt alone. Iâ€™ll include architecture, file-by-file roles, usage examples, and suggested reproduction steps.## Project summary â€” high level âœ…
JobSpy is a Python job-scraper framework that concurrently scrapes multiple job boards (LinkedIn, Indeed, Glassdoor, Google Jobs, Naukri, ZipRecruiter). It standardizes the output into a pandas DataFrame of JobPost records and supports rotating proxies, TLS sessions, and optional richer LinkedIn fetches. It focuses on pragmatic scraping: per-site Scraper implementations, utility session/fingerprint helpers, and a simple runner script.

Key concepts & flow ðŸ”
User calls scrape_jobs(...) (in __init__.py) with:
site(s), search term, location, results_wanted, hours_old, proxies, etc.
scrape_jobs converts parameters to a ScraperInput and runs site scrapers in parallel with ThreadPoolExecutor.
Each site has a Scraper class (e.g., Glassdoor, Indeed), implementing scrape(scraper_input) and returning a JobResponse (list of JobPost).
Utilities handle sessions (TLS/client fingerprinting), rotating proxies, retries, logging, HTML -> Markdown conversion, email extraction, salary parsing, and output column ordering.
Results are normalized and concatenated into a single DataFrame sorted by site/date; the runner writes CSV/Excel output.
Files & structure (what each item does) ðŸ“
main.py â€” example runner / CLI (search + options) that calls scrape_jobs(...) and writes CSV. (Weâ€™ve added site-focused fallback and robust CSV write behavior.)
pyproject.toml / requirements.txt â€” dependency declarations (requests, beautifulsoup4, pandas, pydantic, tls-client, markdownify, regex, etc.)
README.md â€” usage instructions, examples, and project notes (update with your instructions for maintainers).
jobs.csv â€” sample output produced by running main.
jobspy/ â€” main package:
__init__.py â€” scrape_jobs orchestration and mapping to Scraper classes.
model.py â€” Pydantic models / enums: JobPost, JobResponse, Country, Site, DescriptionFormat, Compensation etc.
util.py â€” session utilities (create_session using tls_client or requests), rotating proxy sessions (TLSRotating / RequestsRotating), logging helpers, salary parsing, markdown conversion, and other helpers.
exception.py â€” custom exceptions per-scraper (e.g., GlassdoorException).
Subpackages per site:
glassdoor/ â€” GraphQL call flow, CSRF token extraction, query payloads and headers.

indeed/, linkedin/, google/, naukri/, ziprecruiter/ â€” site-specific scrape logic, constants, and parsing utilities.
Running & reproducing âš™ï¸
Basic: python main.py (or the CLI: python [main.py](http://_vscodecontentref_/42) --search "application support" --sites indeed,linkedin --output out.csv)
If you see 403s:
Try proxies=[...] (residential proxies preferred).
Add browser-like headers (User-Agent, Accept-Language) or enable TLS session mode (via create_session(is_tls=True)).
Consider slower pacing, retry/backoff, or headless browser (Selenium/Playwright) for strong anti-bot measures.
Files to edit for per-site tweaks:
__init__.py â€” change headers, session creation, parsing rules.
util.py â€” tweak create_session or proxy rotation.
Error handling & best practices implemented ðŸ”§
Per-site error capture and status recording (so single-site failures donâ€™t crash the whole process).
CSV write fallbacks: try original filename â†’ timestamped one â†’ temp directory â†’ print rows to stdout and actionable guidance.
Logging via create_logger(name) so runtime logs are grouped by site (e.g., JobSpy:Glassdoor).
Use tls_client for better fingerprinting and avoid some server blocks.
Suggested improvements (priority list) â¬†ï¸
Add unit tests for parsing functions (salary extraction, location parsing).
Add integration tests for per-site scraping with saved HTML fixtures.
Implement automatic exponential backoff and max-retries for 429/5xx responses.
Add optional Playwright/Selenium mode (flag) for sites with JS-based anti-bot protections.
Add config file (YAML/JSON) and a richer CLI (argparse subcommands) for scheduled runs.
Exact "AI prompt" to recreate this repository (copy-paste friendly) ðŸ¤–
Use this as a single prompt to instruct an AI to scaffold the project:

Create a Python package named jobspy that scrapes job boards concurrently and standardizes output into a Pandas DataFrame. Files:

pyproject.toml: dependencies: python >=3.10, requests, beautifulsoup4, pandas, numpy, pydantic, tls-client, markdownify, regex.
main.py: example runner/CLI that calls scrape_jobs(site_name=[...], search_term, location, results_wanted, hours_old, proxies=None), writes CSV with fallbacks if PermissionError occurs. Provide example for 'application support' search in India.
__init__.py: expose function scrape_jobs(site_name, search_term, ... ) which:
maps string names to Site enums,
builds ScraperInput pydantic model,
uses ThreadPoolExecutor to call per-site scraper classes and collects their JobResponse,
normalizes into DataFrame and returns it.
model.py: pydantic models JobPost, JobResponse, enums Site, Country, DescriptionFormat, CompensationInterval.
util.py:
create_session(is_tls=True|False, proxies=None, has_retry=False) returning a session object (use tls_client.Session for TLSRotating, and requests.Session subclass for RequestsRotating with Retry adapter).
create_logger(name) for consistent logging.
rotating proxy helpers, markdown conversion helper, email/salary parsers, and desired_order columns list.
Add a __init__.py for each site (glassdoor, indeed, linkedin, google, naukri, ziprecruiter) with class SiteScraper(Scraper) that:
implements scrape(scraper_input) returning JobResponse(jobs=[JobPost(...)]),
uses create_session and site-specific headers or GraphQL payloads (Glassdoor uses a GraphQL /graph POST and CSRF token; some sites use GET+BeautifulSoup; others may require JS/Playwright).
Implement robust error logging on 403/429/5xx; provide suggestions to use proxies, headers, or headless browser mode.
Also produce a README with usage examples, run commands, dependency install instructions, and notes about 403/workaround strategies."

Ready-to-use README snippet (drop-in) ðŸ“‹
Use this block as README.md content:

JobSpy â€” concurrent job board scraping

Purpose: scrape multiple job boards and produce a canonical DataFrame/CSV of job listings.
Usage: python main.py or python [main.py](http://_vscodecontentref_/78) --search "application support" --sites indeed,linkedin --output out.csv
Notes: If you encounter 403 responses for Glassdoor/Google or other sites, try proxies, browser-like headers, or Playwright/Selenium for sites requiring JS. Some new sites may need the same treatment.
If you want, I can:

create a single file PROJECT_SPEC.md in the repo containing the exact prompt + file-by-file scaffold and example code snippets so that any AI can rebuild it; or
generate automated tests and CI "config to validate scrapers using stored HTML fixtures.
Which would you like me to do next?