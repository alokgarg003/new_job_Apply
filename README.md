

# JobSpy â€” Personalized Job Intelligence ğŸš€

A robust, extensible jobâ€‘scraping and matching pipeline focused on **India** with optional remoteâ€‘job exploration.  
This README walks you through the architecture, how to run it, and how to safely experiment with additional job boards without breaking your reliable sources (LinkedInâ€¯+â€¯Naukri).

## ğŸ“‹ Quick Summary

- **Phaseâ€¯1 â€” Discovery**: scrape job listing URLs from supported boards (LinkedIn, Naukri, Google, Indeed, ZipRecruiter, Glassdoor, RemoteRocketship).
- **Phaseâ€¯2 â€” Enrichment**: fetch full job pages, extract skills/experience/indicators, and score against Alok Gargâ€™s resume.
- **Output**: recruiterâ€‘ready CSV with match score, reasons, missing skills, and alignment level.

## ğŸ“ Project Structure

```
JobSpy/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ publish-to-pypi.yml           # CI to publish to PyPI
â”œâ”€â”€ jobspy/
â”‚   â”œâ”€â”€ __init__.py                       # Public API exports
â”‚   â”œâ”€â”€ model.py                          # Pydantic data contracts (JobPost, ScraperInput, etc.)
â”‚   â”œâ”€â”€ exception.py                      # Custom exceptions
â”‚   â”œâ”€â”€ evaluator.py                      # ProfileMatchEvaluator (resumeâ€‘aware scoring)
â”‚   â”œâ”€â”€ util.py                           # Shared utilities (session, logging, helpers)
â”‚   â”œâ”€â”€ pipeline.py                       # Core discovery + enrichment pipeline
â”‚   â”œâ”€â”€ scrape_jobs.py                    # Public scraping API (concurrent multiâ€‘site)
â”‚   â”œâ”€â”€ google/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ constant.py                   # Google headers & async params
â”‚   â”‚   â”œâ”€â”€ util.py                       # Jobâ€‘info extraction helpers
â”‚   â”‚   â””â”€â”€ google.py                     # Google Jobs scraper
â”‚   â”œâ”€â”€ indeed/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ constant.py                   # Indeed GraphQL query & headers
â”‚   â”‚   â”œâ”€â”€ util.py                       # Jobâ€‘type, compensation helpers
â”‚   â”‚   â””â”€â”€ indeed.py                     # Indeed scraper
â”‚   â”œâ”€â”€ linkedin/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ constant.py                   # LinkedIn headers
â”‚   â”‚   â”œâ”€â”€ util.py                       # Jobâ€‘type, location helpers
â”‚   â”‚   â””â”€â”€ linkedin.py                   # LinkedIn scraper
â”‚   â”œâ”€â”€ naukri/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ constant.py                   # Naukri headers
â”‚   â”‚   â”œâ”€â”€ util.py                       # Jobâ€‘type, remote helpers
â”‚   â”‚   â””â”€â”€ naukri.py                     # Naukri scraper
â”‚   â”œâ”€â”€ ziprecruiter/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ constant.py                   # ZipRecruiter headers
â”‚   â”‚   â”œâ”€â”€ util.py                       # Jobâ€‘type, remote helpers
â”‚   â”‚   â””â”€â”€ ziprecruiter.py               # ZipRecruiter scraper (blocked for India)
â”‚   â”œâ”€â”€ glassdoor/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ constant.py                   # Glassdoor headers & query
â”‚   â”‚   â”œâ”€â”€ util.py                       # Jobâ€‘type, compensation helpers
â”‚   â”‚   â””â”€â”€ glassdoor.py                  # Glassdoor scraper (blocked for India)
â”‚   â”œâ”€â”€ remoterocketship/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ constant.py                   # RemoteRocketship headers
â”‚   â”‚   â”œâ”€â”€ util.py                       # Remoteâ€‘specific helpers
â”‚   â”‚   â””â”€â”€ remoterocketship.py           # RemoteRocketship scraper
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ 01_test_pipeline_validation.py
â”‚       â”œâ”€â”€ 02_test_pipeline_normalize.py
â”‚       â”œâ”€â”€ 03_test_debug_file_and_normalization.py
â”‚       â””â”€â”€ 04_test_write_debug_file.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_discover.py                   # Quick discovery (URLs only)
â”‚   â”œâ”€â”€ run_enrich_debug.py               # Debug enrichment (perâ€‘job scores)
â”‚   â”œâ”€â”€ run_alok.py                       # Full personalized pipeline (default)
â”‚   â”œâ”€â”€ run_alok_remote.py                # Include RemoteRocketship
â”‚   â””â”€â”€ finalize_alok_output.py           # Helper to rename debug dump
â”œâ”€â”€ main.py                               # CLI entry point
â”œâ”€â”€ pyproject.toml                        # Poetry config
â”œâ”€â”€ requirements.txt                      # Pip requirements
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1ï¸âƒ£ Install Dependencies

```bash
# Using pip
pip install -r requirements.txt

# Or using Poetry
poetry install
```

### 2ï¸âƒ£ Run the Default Pipeline (LinkedIn + Naukri)

```bash
python run_alok.py
```

**What it does**
- Searches for jobs matching `["Application Support", "ServiceNow", "IT Support"]` in India.
- Scrapes LinkedIn and Naukri (these are known to work for India).
- Enriches each job with skill extraction and resume-aware scoring.
- Saves `alok_personalized.csv` (final) and a timestamped debug dump.

**Full run (complete job list)**

To run a full pipeline and obtain a larger results set (for example, 200 results), use either the packaged run script or the CLI:

```bash
# Packaged full run (uses 200 results by default and writes `alok_personalized.csv`)
python run_alok.py

# CLI full run with explicit results and output path
python main.py --results 200 --output outputs/full_run.csv
```

Notes:
- Aggregation: when `settings.ENABLE_AGGREGATE_OUTPUT` is True the pipeline will append results to `outputs/all_jobs.csv`.
- Completion message: the pipeline prints a summary like: `Personalized pipeline completed. X jobs saved to <output_file>`.
- Manual append to master (if needed):

```bash
python -c "from jobspy.output_manager import append_to_master; print(append_to_master('outputs/full_run.csv','outputs/all_jobs.csv'))"
```

### 3ï¸âƒ£ Add Remote Jobs (Optional)

```bash
python run_alok_remote.py
```

This includes **RemoteRocketship** in addition to LinkedInâ€¯+â€¯Naukri, giving you remote opportunities without affecting your baseline.

### 4ï¸âƒ£ CLI Usage

```bash
python main.py "Application Support" -l India -s linkedin naukri -r --results 50 --output my_jobs.csv
```

Available sites: `linkedin`, `indeed`, `glassdoor`, `naukri`, `google`, `ziprecruiter`.

**Tip:** For India, stick to `linkedin` and `naukri`. Other sites may return 0 jobs or be blocked.

## ğŸ“Š Output Columns

| Column | Description |
|--------|-------------|
| `title` | Job title |
| `company_name` | Employer name |
| `location` | City, State, Country |
| `site` | Source (linkedin / naukri / remote_rocketship) |
| `job_url` | Direct link to the listing |
| `experience_range` | Extracted years of experience |
| `key_skills` | Skills matched from the description |
| `match_score` | 0â€“100 score against your resume |
| `why_this_job_fits` | Humanâ€‘readable reasons |
| `missing_skills` | Gaps vs your resume |
| `resume_alignment_level` | Strong Match / Good Match / Stretch / Ignore |
| `is_remote` | Whether the job is remote |
| `work_from_home_type` | Remote/Hybrid/Onsite (when available) |

## ğŸ”§ Extending & Experimenting Safely

### A. Keep Your Baseline Intact

The pipeline includes a **safeâ€‘site filter** that only uses LinkedInâ€¯+â€¯Naukri for India. You can run any additional site without affecting this baseline by using the `--remote` flag or passing a custom site list.

### B. Try Other Sites (Optional)

If you want to experiment with Google, Indeed, ZipRecruiter, or Glassdoor:

1. **Force a US location** for those sites to see if they return remote listings:

   ```bash
   # Example: add Google with US location
   python main.py "Application Support" -l India -s linkedin naukri google --remote
   ```

   The code will automatically override the location to â€œUnited Statesâ€ for Google while keeping LinkedInâ€¯+â€¯Naukri on â€œIndiaâ€.

2. **Add a Proxy** for blocked sites (ZipRecruiter, Glassdoor):

   Edit `jobspy/enhance.py` and provide proxies:

   ```python
   def get_proxy_for_site(site):
       proxies = {
           "ziprecruiter": {"http": "YOUR_PROXY", "https": "YOUR_PROXY"},
           "glassdoor": {"http": "YOUR_PROXY", "https": "YOUR_PROXY"},
       }
       return proxies.get(site)
   ```

   Then run with those sites; the scraper will use the proxy.

**Warning:** These sites may still return 0 jobs or errors for India. The safeâ€‘site filter prevents them from breaking your main run.

### C. Add Your Own Site

1. Create a new package under `jobspy/` (e.g., `jobspy/mysite/`).
2. Implement:
   - `constant.py` (headers, query templates)
   - `util.py` (helpers)
   - `mysite.py` (scraper class inheriting `Scraper`)
3. Register it in `jobspy/scrape_jobs.py` under `SCRAPER_MAPPING`.
4. Test with:

   ```bash
   python main.py "Your Query" -s mysite
   ```

## ğŸ§ª Testing Individual Modules

Run each scraper in isolation to see which work:

```bash
# LinkedIn
python -c "from jobspy.pipeline import discover_jobs; jobs = discover_jobs(keywords=['IT Support'], location='India', results_wanted=5, sites=['linkedin']); print(f'LinkedIn: {len(jobs)} jobs')"

# Naukri
python -c "from jobspy.pipeline import discover_jobs; jobs = discover_jobs(keywords=['IT Support'], location='India', results_wanted=5, sites=['naukri']); print(f'Naukri: {len(jobs)} jobs')"

# RemoteRocketship
python -c "from jobspy.pipeline import discover_jobs; jobs = discover_jobs(keywords=['IT Support'], location='India', results_wanted=5, sites=['remote_rocketship']); print(f'RemoteRocketship: {len(jobs)} jobs')"
```

## ğŸ› Troubleshooting

| Symptom | Likely Cause | Fix |
|---------|--------------|-----|
| 0 jobs from Indeed/Google | Regionâ€‘restricted for India | Use `--remote` to force US location or skip these sites |
| 403 from ZipRecruiter/Glassdoor | Cloudflare block | Add a residential proxy or skip them |
| `AttributeError: 'Glassdoor' object has no attribute '_get_csrf_token'` | Incomplete Glassdoor scraper | Skip Glassdoor or provide a working implementation |
| Slow runs | No proxies for blocked sites | Add proxies or remove blocked sites from your list |

## ğŸ“¦ Dependencies

- `requests` â€“ HTTP client
- `beautifulsoup4` â€“ HTML parsing
- `markdownify` â€“ HTML â†’ Markdown
- `pydantic` â€“ Data validation
- `pandas` â€“ CSV output
- `numpy` â€“ Math helpers

## ğŸ¤ Contributing

1. Fork the repository.
2. Add tests under `jobspy/tests/`.
3. Update the README if you add a new site or major feature.
4. Submit a pull request.

## ğŸ“„ License

MIT â€“ see `LICENSE`.

## ğŸ™ Acknowledgments

- The project leverages public jobâ€‘board APIs and HTML structures. Respect the sitesâ€™ robots.txt and terms of service.
- The resumeâ€‘matching logic is tuned for Alok Gargâ€™s profile; you can adjust keywords in `jobspy/evaluator.py`.

---

**Happy hunting! ğŸš€**

---

## ğŸ§‘â€ğŸ’» Developer notes (quick)

- Run a dry (no-network) pipeline for quick verification:
  ```bash
  python main.py --dry --results 2 --output outputs/dry_out.csv
  ```

- Run a short live run to validate scrapers (small results):
  ```bash
  python main.py --results 5 --output outputs/live_test.csv
  ```

- Manual tests (no pytest required):
  ```bash
  python tests/run_manual_tests.py
  ```

- Tests added: basic utils, Naukri parsing, and a dry pipeline smoke test. Use `pytest` if you install it (`pip install pytest`).

- If you want CI friendly tests, I can add GitHub Actions and ensure `pytest` runs on PRs.

